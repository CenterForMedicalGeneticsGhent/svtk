#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright Â© 2015 Matthew Stone <mstone5@mgh.harvard.edu>
# Distributed under terms of the MIT license.

"""
Intersect SV called by cluster-based algorithms.

Paired-end and split-read callers provide a reasonably precise estimation of
an SV breakpoint. This program identifies variant calls that fall within
the expected margin of error made by these programs and clusters them together.
The cluster distance defaults to 500 bp but it is recommended to use the
maximum individual clustering distance across the libraries being analyzed.
(Generally median + 7 * MAD)
"""

import argparse
import heapq
import os
import pkg_resources
from collections import deque
from pysam import VariantFile, TabixFile
from svtools.svfile import SVFile, SVRecordCluster
from svtools.genomeslink import GenomeSLINK


def flatten_pos(records, name, fout):
    for record in records:
        chrom = record.CHROM
        start = record.POS
        end = record.INFO['END']
        quad = record.samples[0].sample.split('.')[0]
        source = record.source
        ID = record.ID
        svtype = record.INFO['SVTYPE']

        entry = [str(x) for x in [chrom, start, end, quad, source, ID, name,
                                  svtype]]
        entry = '\t'.join(entry) + '\n'
        fout.write(entry)


class VCFCluster(GenomeSLINK):
    def __init__(self, svfiles,
                 dist=500, frac=0.0, match_strands=True,
                 blacklist=None, svtypes=None):
        """
        Clustering of VCF records.

        Records are clustered with a graph-based single linkage algorithm.
        Records are linked in the graph if their breakpoints are within a
        specified distance (default 500 bp) and if they share a minimum
        reciprocal overlap (default 0.1; records of type BND are not subjected
        to the reciprocal overlap requirement).

        Parameters
        ----------
        svfiles : list of SVFile
            Standardized VCFs to cluster
        header : pysam.VariantHeader
            VCF header to use when creating new records
        dist : int, optional
            Clustering distance. The starts and ends of two records must both
            be within this distance in order for the records to be linked.
        frac : float, optional
            Minimum reciprocal overlap for two records to be linked.
        match_strands : bool, optional
            If true, two records must share strandedness in order to be linked.
        blacklist : pysam.TabixFile, optional
            Blacklisted genomic regions. Records in these regions will be
            removed prior to clustering.
        svtypes : list of str, optional
            SV classes to be clustered. Records with an svtype not present in
            this list will be removed prior to clustering. If no list is
            specified, all svtypes will be clustered.
        """

        # Merge sorted SV files
        nodes = heapq.merge(*svfiles)

        # Make lists of unique sources and samples to construct VCF header
        sources = set()
        samples = set()

        for svfile in svfiles:
            sources.add(svfile.source)
            samples = samples.union(svfile.reader.header.samples)

        # Build VCF header for new record construction
        self.samples = sorted(samples)
        self.sources = sorted(sources)
        self.header = self.make_vcf_header()

        # Parameterize clustering
        self.frac = frac
        self.match_strands = match_strands
        self.svtypes = svtypes

        super().__init__(nodes, dist, 1, blacklist)

    def filter_nodes(self):
        """
        Filter records before clustering.

        In addition to default removal of records in blacklisted regions,
        filter records if:
        1) Record does not belong to one of the specified SV classes.
        2) Record is marked as SECONDARY
        3) Record is not on a whitelisted chromosome (default: 1-22,X,Y)

        Yields
        ------
        node : svfile.SVRecord
        """

        for node in super().filter_nodes():
            if self.svtypes is not None and node.svtype not in self.svtypes:
                continue
            if 'SECONDARY' in node.record.info:
                continue
            if not node.is_allowed_chrom():
                continue
            yield node

    def cluster(self):
        """
        Yields
        ------
        record : SVRecord
        """
        clusters = super().cluster(frac=self.frac,
                                   match_strands=self.match_strands)
        for records in clusters:
            if len(records) > 0:
                cluster = SVRecordCluster(records)
                record = self.header.new_record()
                record = cluster.merge(record, self.sources)
                yield record, cluster

    def make_vcf_header(self):
        """
        Add samples and sources to VCF template header.

        Returns
        -------
        pysam.VariantHeader
        """

        # Read stock template
        template = pkg_resources.resource_filename(
                        'svtools', 'data/vcfcluster_template.vcf')
        template = VariantFile(template)
        header = template.header

        # Add samples
        for sample in self.samples:
            header.add_sample(sample)

        # Add source FORMAT fields
        meta = ('##FORMAT=<ID={0},Number=1,Type=Integer,'
                'Description="Called by {1}"')
        for source in self.sources:
            header.add_line(meta.format(source, source.capitalize()))

        return header


def parse_filepaths(filepaths):
    """
    Parameters
    ----------
    filepaths : list of str
        List of paths to standardized VCFs

    Returns
    -------
    svfiles : list of SVFile
    """

    svfiles = deque()
    for path in filepaths:
        if len(path.split()) != 1:
            raise ValueError('File list must be single column')
        if not os.path.isfile(path):
            raise FileNotFoundError('VCF {0} not found'.format(path))

        svfile = SVFile(path)
        svfiles.append(svfile)

    return svfiles


def main():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('filelist', type=argparse.FileType('r'),
                        help='List of paths to standardized VCFS')
    parser.add_argument('fout', help='Clustered VCF.')
    parser.add_argument('-c', '--chrom', default=None,
                        help='Restrict clustering to single chromosome. '
                        'Useful for parallelization.')
    parser.add_argument('--start', default=None, type=int,
                        help='Restrict clustering to single chromosome. '
                        'Useful for parallelization.')
    parser.add_argument('--end', default=None, type=int,
                        help='Restrict clustering to single chromosome. '
                        'Useful for parallelization.')
    parser.add_argument('-d', '--dist',
                        type=int, default=500,
                        help='Maximum clustering distance. Suggested to use '
                        'max of median + 7*MAD over samples. [500]')
    parser.add_argument('-r', '--frac',
                        type=float, default=0.0,
                        help='Minimum reciprocal overlap between variants. '
                        '[0.0]')
    parser.add_argument('-m', '--match-strands', default=False,
                        action='store_true')
    parser.add_argument('-x', '--blacklist', metavar='BED.GZ',
                        type=TabixFile, default=None,
                        help='Tabix indexed bed of blacklisted regions. Any '
                        'SV with a breakpoint falling inside one of these '
                        'regions is filtered from output.')
    parser.add_argument('-z', '--svsize', type=int, default=500,
                        help='Minimum SV size to report for intrachromosomal '
                        'events. [0]')
    parser.add_argument('-p', '--prefix',
                        default='MERGED',
                        help='Prefix for merged variant IDs. [MERGED]')
    parser.add_argument('-t', '--svtypes', default=None,
                        help='Comma delimited list of svtypes to restrict '
                        'clustering to (del,dup,inv,tloc)')
    parser.add_argument('--cluster-bed', type=argparse.FileType('w'),
                        help='Bed of constituent calls in each cluster')
    args = parser.parse_args()

    # Parse SV files and lists of samples and sources
    filepaths = [line.strip() for line in args.filelist.readlines()]
    svfiles = parse_filepaths(filepaths)

    # Fetch region of interest
    if args.chrom is not None:
        for svfile in svfiles:
            svfile.fetch(args.chrom, args.start, args.end)

    #  if args.svtypes is not None:
    #      svtypes = args.svtypes.split(',')
    #      permitted_types = 'del dup inv tloc'.split()
    #      for svtype in svtypes:
    #          if svtype not in permitted_types:
    #              raise Exception('Unpermitted svtype: {0}'.format(svtype))
    #  else:
    svtypes = None

    svc = VCFCluster(svfiles, dist=args.dist, blacklist=args.blacklist,
                     frac=args.frac, match_strands=args.match_strands,
                     svtypes=svtypes)
    # Open new file
    fout = VariantFile(args.fout, mode='w', header=svc.header)

    for i, (record, cluster) in enumerate(svc.cluster()):
        # Name record
        if args.prefix:
            name = [args.prefix]
        else:
            name = ['SV']
        if args.chrom:
            name.append(args.chrom)
        name.append(str(i + 1))
        record.id = '_'.join(name)

        fout.write(record)

        #  # Size filter (CTX have size -1)
        #  if -1 < record.INFO['SVLEN'] < args.svsize:
            #  continue

        #  writer.write_record(record)

        #  if args.cluster_bed is not None:
            #  flatten_pos(cluster, record.ID, args.cluster_bed)

    fout.close()

if __name__ == '__main__':
    main()
